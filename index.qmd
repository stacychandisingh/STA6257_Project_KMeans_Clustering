---
title: "K-Means Clustering"
authors: "Stacy Chandisingh, Leo Pena, Shaif Hossain"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
---
## Introduction

The researchers’ main focus for this project is to use an unsupervised machine learning technique called K-means clustering to glean insights from data. Unsupervised machine learning finds hidden data structures from an unlabeled dataset and its aim is to find the similarities within the data groups.  The goal of the k-means clustering technique is to determine the initial optimal centroids of clusters (Zubair et al, 2022). The aim is to group similar data points together into ‘K’ number of clusters based on the similarity of the data or the measure of the distance to a centroid. The centroid is generally the average of the position of all the points on a cluster. 


A few different methods of obtaining the K number of clusters in K-Means Clustering include: the elbow method, by rule of thumb, information criterion approach, and information theoretic approach, and choosing k using the Silhouette, and cross-validation. (Kodinariya et al, 2013). There are different statistical approaches that can be used in a k-means clustering algorithm such as Principal Component Analysis (PCA) amongst others, that help to improve the efficiency and accuracy of a k-means algorithm. One effective way of finding the k-means’ initial centroids and reducing the number of iterations is using principal component analysis (PCA) and percentile; this includes having the data undergo PCA, apply a percentile, split the dataset and find the mean of each attribute, determine the centroids, and finally, cluster generation (Zubair et al, 2022). Other methods involved in a k-means analysis include the hash algorithm or canopy algorithm to improve defining the center cluster (Ardavan et al, 2021).


There are different aspects of the k-means clustering algorithm which are best suited for certain datasets and use cases. This can include the fuzzy k-means algorithm which is combined with other techniques that best prepare and analyze the data to get the most appropriate result for the particular use case. It was observed that k-means algorithms are typically tailored to suit the use case being studied. Fuzzy k-means caters for overlapping datasets in terms of similarity of the dataset features (k-means does not) and is used as part of the process in identifying and diagnosing melanoma skin cancer in patients. A skin cancer detection method involves deep learning and fuzzy k means which can positively impact early skin cancer detection by increasing the accuracy of detection (Nawaz et al, 2022). 


The researchers understand the value of how the selection of the K value can affect the results of a study. The seismic data analyzed in Xiong et al, 2021 highlights that K=5 provides the least stable results while the selection of K=4 is the optimal K as supported by the Elbow Method. A way in which the value of the k-means value can be estimated is by using various tools like the Davies Bouldin (DB) Index where the value for each k was studied. The DB index measures how good the clusters are by assessing low within-cluster variation and high between-cluster separation. The DB index generated for the clusters between a more enhanced k-means algorithm, the Principal Component Analysis-Particle Swarm Optimization (PCA-PSO) K-Means algorithm versus the general K-Means algorithm found that the PCA-PSO-K Means algorithm generated a lower DB index for all the clusters indicating better cluster performance compared to the general K-Means algorithm (Sadeghi et al., 2023). The general framework of the PCA-PSO-K Means algorithm includes computing the mean, centering the data, computing the covariance matrix, computing the eigenvalues and eigenvectors, finding the fraction of total variance, choosing the dimensionality, reducing the basis and dimensionality data, initializing particles and calculating the fitness value (Sadeghi et al, 2023).  


After reviewing multiple peer-reviewed articles, the general trend we observed was the importance of cleaning and preprocessing data to a suitable format for running through a k-means algorithm. There are various ways in how this process can be achieved like using preprocessing techniques available in the R software program such as tidyverse, cluster, and factoextra. Missing values in the dataset should be considered in how they should be treated where they could either be removed or replaced with a value. Variables can be standardized to be made comparable. The Euclidean distances are calculated to get the clustering distances measurement and the distance data can be visualized using the fviz_dist() function in R from the factoextra package. Descriptive statistics and bar graphs can also be generated to visualize the spread of the data (Abdullah et al, 2022). The researchers set out to explore the k-means clustering algorithm.



## Individual Research Summaries for K-Means Clustering

# Week 2:

**Articles read this week and their summaries:**

**Summary 1:**

K-Means Clustering is a type of unsupervised machine learning used for multivariate datasets. The aim is to group similar data points together into ‘K’ number of clusters based on the similarity of the data or the measure of the distance to a centroid. The centroid is generally the average of the position of all the points on a cluster. This article goes on to explore six different methods of obtaining the K number of clusters in K-Means Clustering which include: the elbow method, by rule of thumb, information criterion approach, and information theoretic approach, choosing k using the Silhouette, and cross-validation. 

The article generally states that the elbow method uses a visual approach in determining the K where a graph is viewed to see where there is a pivot on the plot. The point which corresponds to the value on the x-axis that resembles the point of an elbow is used as the K value. The rule of thumb approach uses the number of data points as the K-value. The information criterion approach balances the complexity of a model and goodness of fit of data by employing Akaike’s information criterion (AIC), the consistent Akaike’s information criterion (CAIC), and the Bayesian inference criterion (BIC). The Information Theoretic Approach uses concepts from rate distortion theory where jump statistics is also introduced, and the maximum jump determines the correct number of clusters. K can also be chosen using the average Silhouette width of individual entities where the largest average silhouette width over different K, results in the most appropriate number of clusters. The article discusses its final method for determining the number of K using cross-validation where the data is split into two parts where one part is used for clustering and the other part is used for validation and a final K is determined from the appropriate algorithmic steps to determine the K in K-Means Clustering.

Kodinariya, Trupti M., Makwana, Dr. Prashant R. (2013). Review on determining number of Cluster in K-Means Clustering. International Journal of Advance Research in Computer Science and Management Studies. Volume 1, Issue 6, 90-95. https://www.researchgate.net/profile/Trupti-Kodinariya/publication/313554124_Review_on_Determining_of_Cluster_in_K-means_Clustering/links/5789fda408ae59aa667931d2/Review-on-Determining-of-Cluster-in-K-means-Clustering.pdf

**Summary 2:**

This article discusses the power of K-Means Clustering and generally states the usefulness and popularity of this unsupervised machine learning technique. The article describes the concern for this technique which is to determine the initial optimal centroids of clusters. It goes on to propose an approach to find the iteration number and reduce the number of iterations. Unsupervised machine learning finds hidden data structures from an unlabeled dataset and its aim is to find the similarities within the data groups.

The article discusses various approaches taken on by others to find the initial cluster centroids more efficiently and a few of these include: a centroids selection method based on radial and angular coordinates; finding initial centroids based on the dissimilarity tree; a new weighted average approach to finding the initial centroids by calculating the mean distance of every data point’s distance; dividing the sorted distances with k, the number of equal partitions; the nearest neighbor method; the distance of the neighborhood; proposal to save the distance to the nearest cluster of the previous iteration and used the distance to compare in the next iteration; and the farthest distributed centroids clustering (FDCC) algorithm. The article mentions various drawbacks and lack of further information surrounding these methods and goes on to propose a more effective way of finding the k-means’ initial centroids and reducing the number of iterations using principal component analysis (PCA) and percentile. The researchers of the article demonstrate this approach by using a real-world COVID-19 dataset. A general overview of the article’s more efficient proposal includes having the data undergo PCA, apply a percentile, split the dataset and find the mean of each attribute, determine the centroids, and finally, cluster generation.

Zubair, M., Iqbal, M.A., Shil, A. et al. An Improved K-means Clustering Algorithm Towards an Efficient Data-Driven Modeling. Ann. Data. Sci. (2022). https://doi.org/10.1007/s40745-022-00428-2

**Summary 3:**

**The Systemic Review of K-Means Clustering Algorithm**

This article explains versions of the current k-means clustering algorithm, current issues and limitations in the process, and what changes have been made to improve upon these issues. In explaining the algorithm, there is a step-by-step explanation of the traditional process. The issues and limitations mentioned are in defining the value of k, finding the initial center, reducing noise, and handling big datasets. In k-means clustering, you must define the number of clusters prior to the executing the algorithm, which is the biggest issue in dynamic datasets that change over time. The more effective you are at picking the cluster center, the less iterations you need in the process with a higher accuracy in your clusters. However, the more random the cluster center increases the randomness of the clusters, and this gets more difficult with larger and complex data structures used by major firms. Finally, noise is an issue in k-means clustering as it effects each iteration of cluster, with a heavy impact on the final cluster result. To fix the noise issue, it is assumed that determining the best possible number of clusters and finding the ideal center will reduce the noise effecting the algorithm.  There are new methods such as the hash algorithm or canopy algorithm to improve defining the center cluster. This article was a review of k-means clustering, and studies confirming the main issues surrounding the process. With our main concerns identified, we know which issues to focus on when conducting our research. 


Ardavan Ashabi, Shamsul Bin Sahibuddin, and Mehdi Salkhordeh Haghighi. 2021. The Systematic Review of K-Means Clustering Algorithm. In Proceedings of the 2020 9th International Conference on Networks, Communication and Computing (ICNCC '20). Association for Computing Machinery, New York, NY, USA, 13–18. https://doi-org.ezproxy.lib.uwf.edu/10.1145/3447654.3447657

**Summary 4:**

**Adapting the right measures for K-Means Clustering**

Previous performance validation measures are inconsistent and there is not a best available method for validating. This article studied 16 validation methods for K-means clustering, comparing the views, and explaining the defective results that may happen in the process. Some of the positive researched methods include data mining, information retrieval, machine learning, and statistics, as these can all be evaluated through the contingency matrix, which shows the frequency distribution between multiple variables, comparing the “purity” of the clusters. Some of the negative methods include the uniform effect, where the clusters are created of uniform distance from the centroid. The issues with this measure are the entropy measure (the measure of randomness), mutual information, and purity of the clusters, and are defective measures for validating k-means clusters. To improve these problems, the author normalizes the measures statistically, stating a term is valid if it has a certain value against the baseline distribution of the clusters. Finally, the author shows the math and visualization comparisons from normalized and unnormalized measures. The properties he compared were the sensitivity, the impact of the number of clusters, and summaries of the math properties of the datasets. Normalization is the necessary improvement when validating the k-means clustering algorithm. 

Junjie Wu, Hui Xiong, and Jian Chen. 2009. Adapting the right measures for K-means clustering. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '09). Association for Computing Machinery, New York, NY, USA, 877–886. https://doi-org.ezproxy.lib.uwf.edu/10.1145/1557019.1557115


# Week 3:

**Summary 1:**

Fuzzy k-means clustering is related to k-means clustering with the only difference being that a data point can overlap into more than one cluster whereas k-means indicates that a data point is contained in only one cluster. The word “fuzzy” connotates the overlap. This article discusses the important role technology plays in identifying and diagnosing melanoma skin cancer in patients. However, there are still challenges with the technology as image detection is used for the cancer identification and coloration issues can affect the ability to clearly distinguish between a melanoma mole and a non-melanoma affected area on the skin via skin lesion analysis. The article discusses the collective use of the region-based convolutional neural networks (RCNN) deep learning approach combined with fuzzy k-means clustering to develop an automated method in segmenting the early stages of skin melanoma. The clinical images are pre-processed to enhance the visuals of the image data followed by the application of faster RCNN. Fuzzy-k means is applied to the data with limitations in place to further define the melanoma affected skin. The authors tested their proposed method on three different datasets and found accuracies within the ninetieth percentile which did better than current melanoma image detection technologies.

The article dives into various melanoma detection methods explored by other researchers including handcrafted features-based techniques, and segmentation-based techniques like adaptive thresholding and iterative selection thresholding. However, there were drawbacks to these studied approaches. Deep learning was shown to be a helpful approach in enhancing the technique. The researchers of the article in scope of this summary, try to overcome algorithm limitations like computational expense and model overfitting by using their method. An overview of the steps involved in their proposed method include: image pre-processing performed by enhanced image illumination to reduce noise, selecting the best features to use for the analysis using the deep learning faster-RCNN method, and then the segmentation of healthy skin lesions versus melanoma-affected skin lesion is done by fuzzy k-means analysis which accommodates overlapping datapoints and is more appropriate for this use-case rather than k-means clustering which hard codes a data point in only one cluster and does not cater for nuanced or overlapping differences for the particular use-case. 

The researchers conclude their new skin cancer detection method involving deep learning and fuzzy k means can positively impact early skin cancer detection by increasing the accuracy of detection. They also reiterate their model performs better than skin cancer detection models currently on the market.


Nawaz, M., Mehmood, Z., Nazir, T., Naqvi, R. A., Rehman, A., Iqbal, M., & Saba, T. (2022). Skin cancer detection from dermoscopic images using deep learning and fuzzy k-means clustering. Microscopy Research and Technique, 85(1), 339–351. https://doi-org.ezproxy.lib.uwf.edu/10.1002/jemt.23908

**Summary 2:**


The focus of this article is on recorded seismic data in the Southern California plate boundary region, where velocity models are analyzed using k-means clustering to uncover any hidden features that can be found by analyzing clusters of the velocity model data. The study focuses on an alternative method to rate a velocity model using the K-means clustering method. This technique is applied to community velocity models (CVM) using Rayleigh wave phase velocity maps, and two frequently used datasets, CVM-H15.1 and CVM-S4.26, which are often used in tomography studies. 

Theoretical predictions are made for a measured parameter, phase velocity of Rayleigh wave, and the k-means model is applied and compared with clustering results obtained for synthetic and observed data sets.

A collection of 1-D velocity curves is grouped into a predetermined number of clusters. The researchers describe how they prepare the data to undergo k-means. They also note that the data analysis is sensitive to the K that is chosen and highlights the Elbow Method as a robust source for obtaining an appropriate K. The synthetic surface wave velocity dispersion curves for all 1-D velocity profiles of an input velocity model are calculated first, followed by clustering the synthetic and observed velocity dispersion curves independently into a certain number of groups through k-means clustering. The velocity model is rated by approximating the similarity between spatial patterns gleaned from the synthetic and observed dispersion data.

The Elbow Method was used during the clustering of the observed phase velocity maps and the optimal k was found to be 4. The optimal k of 4 was used in the synthetic phase velocity maps for CVM-H15.1 and CVM-S4.26.

The article discusses how the selection of the K value ranging from 3 to 5 affects the results of the velocity maps and highlights that K=5 provides the least stable results while the selection of K=4 is the optimal K as supported by the Elbow Method. 

The researchers highlight that they developed a workflow to assess velocity models using the K-means clustering approach for the first time by comparing the synthetic phase velocity maps with the observed phase velocity maps. A Jaccard similarity coefficient which is a value used for determining the similarity and diversity of sample sets and this value was at a high 57.4% for CVM-S4.26, which was higher than the 18.6% obtained for CVM-H15.1. Per the K-Means analysis, this suggests that the clusters obtained from CVM-S4.26 match much better with that of the observed data than CVM-H15.1. 

The model proposed in the study seems to be an initial first-order study aimed to complement more advanced model validation studies and seeks to provide information on improving the development of tomographic studies.

Xiong, N., Qiu, H., & Niu, F. (2021). Data-driven velocity model evaluation using K-means clustering. Geophysical Research Letters, 48, e2021GL096040. https://doi-org.ezproxy.lib.uwf.edu/10.1029/2021GL096040

**Summary 3:**

**A New Method for initializing the K-Means clustering Algorithm**

K-Means clustering is very popular and used in pattern recognition and machine learning by selecting initial cluster centers and determining the distance of a cluster from the centroid for validation. One of the problems this author attempts to fix is the random selection of the initial centroid for a better set of clusters and stronger partitioning. Some of the issues with k-means clustering are that the algorithm assumes the number of clusters are known beforehand, that the algorithm is sensitive to the initial cluster center, and that it converges finitely to a local minima. A new initialization algorithm is proposed with these issues in mind. In the new algorithm, we choose small subsamples of the data and reassign the centers to the sup samples. This avoids empty clusters that lead to bad solutions. The method was used on evenly distributed data for comparison against the random selection of the initial center and the results showed improvement in effectiveness when comparing the initial points to the true solutions. It is important to note that these results were from a Gaussian dataset, which was also small compared to data collected from major firms, and the scalability of this method was unknown. 

X. Qin and S. Zheng, "A New Method for Initializing the K-Means Clustering Algorithm," 2009 Second International Symposium on Knowledge Acquisition and Modeling, Wuhan, China, 2009, pp. 41-44, doi: 10.1109/KAM.2009.20.

**Summary 4:**

**A Simple and Fast Algorithm for Global K-Means Clustering**

K-means clustering is used for data mining, pattern recognition, decision support, machine learning, and image segmentation, however the normal algorithm heavy reacts to the initial chosen centroid and does not work well with large datasets. The global K-means algorithm adds one cluster center at a time through a global search procedure where N (with N being the size of the data set) is the number of executions in completed to find the ideal initial center. This article a new algorithm is presented based off the global K Means procedure and incorporates ideas from the K-medoids clustering. The new algorithm has greater performance, reduced computational times, and less influence of noise on its used test data. 

The Global K-means algorithm does not rely on an initial center, and starting with one cluster, finds the best centroid compared to the center in the dataset. Next, the job is completed with 2 clusters and continues through the dataset until the best cluster is available as the answer to your problem. The author of the article trimmed the global algorithm down, ensured it does not depend on initial conditions and sets an optimal center at each stage, significantly reducing the computational load and making it fit for larger datasets. 

J. Xie and S. Jiang, "A Simple and Fast Algorithm for Global K-means Clustering," 2010 Second International Workshop on Education Technology and Computer Science, Wuhan, China, 2010, pp. 36-40, doi: 10.1109/ETCS.2010.347.


# Week 4:

**Summary 1:**

This article focuses on proposing an algorithm incorporating various algorithm techniques in their machine learning analysis of customer data. Specifically, their focus is customer churn prediction which is an aspect of Customer Relationship Management that determines which customer is likely to leave a service or cancel a subscription-based service. The paper proposes an algorithm that combines Principal Component Analysis (PCA), K-Means and Particle Swarm Optimization (PSO) to improve the accuracy of determining customer churn prediction. The analysis focuses on the challenges of customer retention in the competitive telecommunications industry and uses a dataset related to a fixed internet provider of Iran’s Isfahan Province. PCA is used to select the best features to use in the analysis, K-means helps in the classification process, and PSO enhances the K-Means by providing initial centroids. The algorithm is ultimately called PCA-PSO-K Means algorithm. 

The general framework of the PCA-PSO-K Means algorithm includes computing the mean, centering the data, computing the covariance matrix, computing the eigenvalues and eigenvectors, finding the fraction of total variance, choosing the dimensionality, reducing the basis and dimensionality data, initializing particles and calculating the fitness value. The researchers use the Cross-Industry Standard Process for Data Mining (CRISP-DM) when predicting customer churn. When analyzing and testing the number of clusters which ranged between 2 to 9, the Davies Bouldin (DB) Index for each k was studied. The DB index measures how good the clusters are by assessing low within-cluster variation and high between-cluster separation. The values generated for the clusters between a PCA-PSO-K Means algorithm versus the general K-Means algorithm found that the PCA-PSO-K Means algorithm generated a lower DB index for all the clusters indicating better cluster performance compared to the general K-Means algorithm. The researchers show that the proposed PCA-PSO-K Means algorithm yielded an accuracy of 99.77%, a sensitivity of 75%, a specificity of 99.81% and a correlation coefficient of 0.443±0.271. However, the researchers attribute limitations of their analysis to data reliability and availability. 

Sadeghi, M., Dehkordi, M.N., Barekatain, B. et al. Improve customer churn prediction through the proposed PCA-PSO-K means algorithm in the communication industry. *J Supercomput* 79, 6871–6888 (2023). https://doi-org.ezproxy.lib.uwf.edu/10.1007/s11227-022-04907-4

**Summary 2:**

The goal of article was to cluster Indonesian provinces based on the risk of the COVID-19 pandemic. The provinces were grouped by confirmed, death, and recovered COVID-19 cases using data from the Indonesian COVID-19 Task Force. The researchers implemented the use of the K-Means Clustering algorithm where clustering generated three provincial groups whereby provinces are grouped together based on similarity of either confirmed, death, and recovered cases. This information sets out to inform government decisions in implementing strategies to reduce the spread of COVID-19.

Using the K-Means method where similarities between groups are established based on distance vectors, the researchers seek out to pre-process their dataset to undergo the steps involved in a K-Means analysis. The researchers provide a step-by-step account of how they preprocessed their dataset using the R software program including the R packages used such as tidyverse, cluster, and factoextra. Missing values in the dataset were removed and variables were standardized to be made comparable. The Euclidean distances were calculated to get the clustering distances measurement and the distance data was visualized using the fviz_dist() function in R from the factoextra package. Descriptive statistics and a bar graph were also generated for the categories: confirmed, recovered, and death cases to determine and visualize the spread of the data.

The researchers go on to determine the number of clusters using three top optimal clustering methods including the Elbow method, the Silhouette method, and the Gap statistic method. The researchers point out that the k value is found from the fviz_nbclust function of each method and describes the differences of how the k value is visualized from each method. The k value from the Elbow method is observed from the largest dip on the visualization graph, while it is automatically generated for the Silhouette and Gap statistic graphs. The researchers go on to describe how the number of clusters was used to calculate the k-means clustering value and provide code to visualize the k-means clustering result.

The researchers concluded that two clusters were optimal for this analysis based on the three optimal clustering methods. The results of the study determined there are 3 clusters of provinces with the province of Jakarta being in its own group as a third cluster as Jakarta had more cases. Jakarta was excluded from the data clustering analysis. Cluster 1 consisted of 5 provinces and Cluster 2 consisted of 28 provinces. The provincial clustering data will help the government establish policies to overcome the spread of COVID-19.


Abdullah, Susilo, S., Ahmar, A. S., Rusli, R., & Hidayat, R. (2022). The application of K-means clustering for province clustering in Indonesia of the risk of the COVID-19 pandemic based on COVID-19 data. *Quality & Quantity*, 56(3), 1283–1291. https://doi.org/10.1007/s11135-021-01176-w

**Summary 3:**

**An Efficient K-Means Clustering Algorithm: Analysis and Implementation**

The algorithm in this article is known as a filtering algorithm, in which it is based on storing data points from a KD-tree, which is a space partitioning data structure. You start with computing the kd-tree and then determining the initial centers (there is not a best practice for finding the center mentioned in this article). For every node there is a set candidate centers, and we filter out those whose center is farthest from a given Z, your chosen initial center. The analysis of time saved shows how further separation between the clusters improves the run time of the algorithm. This was practiced on synthetic data with experiments involved color quantization, vector quantization, and image segmentation. 	The filtering algorithm outperformed competing clustering algorithms, although is noted that the data set likely needs to be clustering friendly. One of the issues is that information is not passed down, so there is not a stage of the process in which the clusters closest centers are comparable (which could further improve processing time). There is work to be done in further improvement, however the filtering algorithm accomplished its task when compared to separate clustering methods. 

T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman and A. Y. Wu, "An efficient k-means clustering algorithm: analysis and implementation," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 7, pp. 881-892, July 2002, doi: 10.1109/TPAMI.2002.1017616

**Summary 4:**

**A Fuzzy k-Modes Algorithm for Clustering Categorical Data**

Fuzzy K-means algorithm means each data point is allowed to have functions in all the clusters rather than just one cluster. This article uses the fuzzy k means algorithm on categorical data by matching dissimilar features for categorical objects. This can show a transition between zones and build a practical story from the associated patterns. The authors of the article chose multiple data set to determine the performance of the algorithm. On a soybean dataset in which all columns held categorical data, three clustering algorithms were used to create four clusters and ran 100 times. The number of runs with correct classifications were higher for the fuzzy algorithm compared to a standard k means clustering algorithm, and the average computing time was much smaller for our chosen method. The next experiment measured efficiency as compared to a hard k-modes algorithm as it has been shown to work on large data sets, which is an issue with standard k means. An artificial dataset was used to test fuzzy k means by splitting into two clusters of 5000 objects. When measuring the dissimilarity using the categorical variables from the dataset it was found that hierarchal clustering was like fuzzy in terms of accuracy, however the fuzzy algorithm was much faster in completing it. So, when the dataset was large it is better to go with the fuzzy method.  

Zhexue Huang and M. K. Ng, "A fuzzy k-modes algorithm for clustering categorical data," in IEEE Transactions on Fuzzy Systems, vol. 7, no. 4, pp. 446-452, Aug. 1999, doi: 10.1109/91.784206.

**Please ignore information below this line. Website under construction.**

## Methods


The common non-parametric regression model is $Y_i = m(X_i) + \varepsilon_i$, where
$Y_i$ can be defined as the sum of the regression function value $m(x)$ for $X_i$.
Here $m(x)$ is unknown and $\varepsilon_i$ some errors. With the help of this definition, we can create the estimation for
local averaging i.e. $m(x)$ can be estimated with the product of $Y_i$ average
and $X_i$ is near to $x$. In other words, this means that we are discovering
the line through the data points with the help of surrounding data points.
The estimation formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$
$W_n(x)$ is the sum of weights that belongs to all real numbers. Weights
are positive numbers and small if $X_i$ is far from $x$.

## Analysis and Results
### Data and Vizualisation
A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```


```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

### Conlusion

## References
